{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fqWveqJEQXJ",
    "nbsphinx": "hidden",
    "outputId": "b07e829b-2a6e-444e-ce98-547c4c58fae5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: md-mermaid in /opt/miniconda3/envs/equadratures/lib/python3.9/site-packages (0.1.1)\n",
      "Requirement already satisfied: markdown>=2.5 in /opt/miniconda3/envs/equadratures/lib/python3.9/site-packages (from md-mermaid) (3.3.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/miniconda3/envs/equadratures/lib/python3.9/site-packages (from markdown>=2.5->md-mermaid) (4.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/miniconda3/envs/equadratures/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.5->md-mermaid) (3.7.0)\n",
      "Requirement already satisfied: markdown in /opt/miniconda3/envs/equadratures/lib/python3.9/site-packages (3.3.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/miniconda3/envs/equadratures/lib/python3.9/site-packages (from markdown) (4.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/miniconda3/envs/equadratures/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown) (3.7.0)\n"
     ]
    }
   ],
   "source": [
    "# Check if running on colab\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "# pip install any missing packages\n",
    "if IN_COLAB:\n",
    "  !pip install equadratures # for standard pip install                                  \n",
    "\n",
    "!pip install md-mermaid\n",
    "!pip install markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Oos-lpfEQXI"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "For nearly five years now, equadratures [1], a pure python code, has been made openly available to the computational methods community. Over the last two years alone, it has been downloaded well over 40,000 times, and is being used across industry, government, and academia. Today, equadratures is a model development platform facilitating explainable and robust models that do not require large cloud infrastructure to train, unlike many deep learning frameworks. Models built in equadratures can be used for a wide range of tasks spanning uncertainty quantification, sensitivity analysis, numerical integration, optimisation, clustering, parameter-space exploration, dimension reduction, surrogate modelling, and even digital twinning.\n",
    "\n",
    "When originally developed five years ago, equadratures (known then as Effective Quadratures) was purpose-built for facilitating non-intrusive uncertainty quantification through polynomial chaos expansions [2–4]. The unique selling point of the code was an adoption of least squares- and compressed sensing-based approaches for estimating polynomial coefficients, rather than opting for more conventional tensor and sparse grid strategies. This permitted greater anisotropy in the selection of basis functions used in the polynomial expansions, as well as a reduction in the number of samples required with rising problem dimensionality. The overall workflow can best be summarised in the steps below.\n",
    "\n",
    "<!-- A **parameter** is one of the main building blocks in Effective Quadratures. Let $s$ be a parameter defined on a domain $\\mathcal{D} \\in \\mathbb{R}$. The support of the domain $\\mathcal{D}$ may be:\n",
    "\n",
    "* closed $[a,b]$\n",
    "* semi-infinite $(-\\infty, b)$ or $[a, \\infty)$\n",
    "* infinite $(-\\infty, \\infty)$\n",
    "\n",
    "Further, let us assume that this parameter is characterized by a positive weight function $\\rho(s)$, which may be interpreted as the probability density function (PDF) of $s$, which readily implies that\n",
    "\n",
    "$$\n",
    "\t\\int_{\\mathcal{D}}\\rho\\left(s\\right)ds=1.\n",
    "$$\n",
    "\n",
    "We now demonstrate some basic functionality of this parameter. First consider the case where $\\rho(s) = \\mathcal{N} (0, 1)$ is a standard Gaussian distribution with a mean of 0.0 and a variance of 1.0. We then plot its PDF and cumulative density function (CDF) and demonstrate how we can generate random samples from this distribution. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "          User prescribes input marginal distributions for each uncertainty.\n",
    "\n",
    "                                         │\n",
    "                                         ▼\n",
    "\n",
    "Least squares or compressed sensing framework is used to generate a design of experiment.\n",
    "\n",
    "                                         │\n",
    "                                         ▼\n",
    "\n",
    "                   Model is evaluated at the design of experiment.\n",
    "\n",
    "                                         │\n",
    "                                         ▼\n",
    "\n",
    "                        Polynomial coefficients are computed.\n",
    "\n",
    "                                         │\n",
    "                                         ▼\n",
    "\n",
    "     Moments and sensitivities (Sobol' indices) are determined from the coefficients.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "$.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js')",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Javascript\n",
    "\n",
    "Javascript(\n",
    "    \"\"\"$.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js')\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Title</h1>\n",
       "<p>Some text.</p>\n",
       "<div class=\"mermaid\">\n",
       "graph TB\n",
       "A --> B\n",
       "B --> C\n",
       "</div>\n",
       "\n",
       "<p>Some other text.</p>\n",
       "<div class=\"mermaid\">\n",
       "graph TB\n",
       "D --> E\n",
       "E --> F\n",
       "</div>\n",
       "\n",
       "<script>mermaid.initialize({startOnLoad:true});</script>\n",
       "<script src=\"mermaid.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import markdown\n",
    "import md_mermaid\n",
    "\n",
    "text = \"\"\"\n",
    "# Title\n",
    "\n",
    "Some text.\n",
    "\n",
    "​```mermaid\n",
    "    graph TB\n",
    "        A --> B\n",
    "        B --> C\n",
    "```\n",
    "\n",
    "Some other text.\n",
    "\n",
    "```mermaid\n",
    "    graph TB\n",
    "        D --> E\n",
    "        E --> F\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "html = markdown.markdown(text, extensions=['md_mermaid'])\n",
    "\n",
    "\n",
    "# req = '\\n<script src=\"https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js\"></script>'\n",
    "req = '\\n<script src=\"mermaid.min.js\"></script>'\n",
    "html += req\n",
    "# print(html)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(html))\n",
    "\n",
    "outfile = open(\"test.html\", 'w')\n",
    "outfile.write(html)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"600\"\n",
       "            src=\"https://www.google.co.uk\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f7d882fdf40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='https://www.google.co.uk', width=700, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shift towards least squares and compressed sensing is well captured in literature [7, 8] as are the different strategies for arriving at well conditioned matrices and sampling distributions [9–11]. There is a clear trend to opt for random sampling techniques paired with well-worn algorithms for identifying well-conditioned submatrices, e.g., QR with column pivoting and SVD-based subset selection [12], among other convex optimisation rooted techniques (see [13] for a comparison).\n",
    "\n",
    "Over the past few years, equadratures has grown in capability, ingesting and synthesising key advances in literature to accelerate application impact. One of the most fruitful advances have been in parameter-space dimension reduction, where the central idea is to ascertain whether a function admits a dimension reducing subspace—i.e., a few linear combination of all the variables which may be used for function approximation. While the idea is itself not unprecedented [14], it has experienced a resurgence owing to a prodigious number of publications under the handles of active subspaces [15], sufficient dimension reduction [16] and ridge functions [17] (to name a few). These works have been championed by researchers spanning both academia and industry—with impactful use cases [18–21] that likely serve as a precursor to further advances within the areas of function approximation. A practical outlook on the success of data-driven dimension reduction in computational science may be enforced by the notion that we trust our models within a relatively small parameter space and conduct our parameter studies accordingly. Thus, function values around a notional centroid may be well-approximated via linear projections of neighboring parameter values.\n",
    "\n",
    "Beyond dimension reduction, ancillary progress on robust methods for dealing with correlations in the inputs, i.e., identifying independent polynomial basis on correlated spaces [22] has been important for driving forth the uptake of polynomial-based methods and thus equadratures. This builds upon prior work with Nataf and Rosenblatt transformations [23]. These advances have permitted the construction of stable global polynomials across both high order and high dimensional problems. This naturally leads one to consider leveraging polynomials across a wider range of challenges including optimisation, multi-fidelity modelling, spatial-field modelling and dimension reduction.\n",
    "\n",
    "It is important to remark that global smoothness and continuity are certainly not guaranteed for all problems. Thus, strategies to fit multiple polynomials over a domain are extremely useful, especially when working with problems that are characterised by a relatively large parameter space. This may take the form of trust region methods [24] or tree-based methods [25], where polynomials need to be defined over a subdomain in a recursive manner—based on certain approximation error criterion. Within equadratures these ideas have found utility in polynomial regression tree models and trust-region optimisation methods. In fact for the latter, if one further assumes that a subspace-based dimension reduction representation exists, then from the perspective of optimiser, as the trust region migrates through a larger parameter space, finding such projections iteratively may be incredibly valuable, both from the perspectives of convergence rate and optimality (see [26]).\n",
    "\n",
    "The rather terse review above sets the stage for our paper, which has two main goals. First, we want to communicate how the building blocks in equadratures—i.e., classes and functions—are aligned with the underpinning mathematics these utilities encapsulate, providing a rich prototyping framework for novice users and experienced practitioners. Second, we wish to demonstrate how problems well-beyond the standard uncertainty quantification mould can be adequately addressed by using equadratures. The remainder of this paper is structured as follows. The remainder of this paper is structured as follows. In II we introduce the core building blocks in equadratures. This is followed by III where we outline strategies for computing coefficients, with gradient enahncement, and under correlations. Section IV details how equadratures exploits parameter subspace-based dimension reduction approaches. This may be used to efficiently compute moments and sensitivities V particularly for high dimensional problems. Techniques for constructing piecewise polynomials using trees is presented in VII, followed by case studies in VIII."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "name": "1_Introduction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
