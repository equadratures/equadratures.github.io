
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Deep learning via polynomials &#8212; equadratures</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../_static/eq-logo-favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main">
<div class="container-xl">

    <a class="navbar-brand" href="../index.html">
    
      <img src="../_static/logo_new.png" class="logo" alt="logo" />
    
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-menu" aria-controls="navbar-menu" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar-menu" class="col-lg-9 collapse navbar-collapse">
      <ul id="navbar-main-elements" class="navbar-nav mr-auto">
        
        
        <li class="nav-item ">
            <a class="nav-link" href="tutorials.html">Tutorials</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="documentation.html">Documentation</a>
        </li>
        
        <li class="nav-item ">
            <a class="nav-link" href="references.html">Research</a>
        </li>
        
        
        <li class="nav-item">
            <a class="nav-link nav-external" href="https://discourse.equadratures.org/">Discourse<i class="fas fa-external-link-alt"></i></a>
        </li>
        
      </ul>


      <form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search equadratures..." aria-label="Search equadratures..." autocomplete="off" >
</form>
      

      <ul class="navbar-nav">
        
          <li class="nav-item">
            <a class="nav-link" href="https://github.com/Effective-Quadratures/equadratures" target="_blank" rel="noopener">
              <span><i class="fab fa-github-square"></i></span>
            </a>
          </li>
        
        
          <li class="nav-item">
            <a class="nav-link" href="https://twitter.com/equadratures" target="_blank" rel="noopener">
              <span><i class="fab fa-twitter-square"></i></span>
            </a>
          </li>
        
      </ul>
    </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
          <div class="col-12 col-md-3 bd-sidebar"><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">

    <div class="bd-toc-item active">
    
  
    <ul class="nav bd-sidenav">
        
        
        
        
        
        
        
      </ul>
  
  </nav>
          </div>
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
              

<nav id="bd-toc-nav">
    <ul class="nav section-nav flex-column">
    
    </ul>
</nav>


              
          </div>
          

          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="deep-learning-via-polynomials">
<h1>Deep learning via polynomials<a class="headerlink" href="#deep-learning-via-polynomials" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial, we extend the generality of our model by studying neural network architectures based on polynomials. A neural network is formed by one or more <em>hidden layers</em>, which are composed of multiple nodes called <em>perceptrons</em>. A perceptron takes in a linear combination (<span class="math notranslate nohighlight">\(\mathbf{w}\)</span>) of the input (<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>) and passes it through a non-linear transformation (<span class="math notranslate nohighlight">\(p\)</span>). The latter is called the <em>activation function</em> of the perceptron. The following figure illustrates the structure of a perceptron.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/perceptron.png"><img alt="../_images/perceptron.png" src="../_images/perceptron.png" style="width: 652.0px; height: 275.0px;" /></a>
</div>
<p>Common activation functions include the sigmoid function, hyperbolic tangent function, and the rectified linear unit (ReLU). In this implementation, we use orthogonal polynomials as the activation function. In practice, perceptrons are usually connected together in multiple hidden layers, similar to the illustration below:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/nn.png"><img alt="../_images/nn.png" src="../_images/nn.png" style="width: 551.0px; height: 256.0px;" /></a>
</div>
<p>The number of perceptrons in each layer, and the connectivity between the perceptrons can vary in practice. In the case of a single hidden layer, the model takes the form of a “multi-ridge function” with polynomials,</p>
<div class="math notranslate nohighlight">
\[y = p_1(\mathbf{w}_1^T \mathbf{x}) + p_2(\mathbf{w}_2^T \mathbf{x}) + ... + p_n(\mathbf{w}_n^T \mathbf{x}).\]</div>
<p>In this model, the parameters are given by the ridge directions <span class="math notranslate nohighlight">\(\mathbf{w}_i\)</span> (or “weights”) in each perceptron, together with the polynomial coefficients of each <span class="math notranslate nohighlight">\(p_i\)</span>. Given some data <span class="math notranslate nohighlight">\((\mathbf{x}^{(i)}, t^{(i)})_{i=1}^N\)</span>, how do we fit the parameters? We can minimize the mean squared error inferred from our training data,</p>
<div class="math notranslate nohighlight">
\[L = \sum_{i=1}^N (t^{(i)} - y(\mathbf{x}^{(i)}))^2\]</div>
<p>A wealth of optimization techniques can be found in the neural networks literature. A technique known as <em>error backpropagation</em> allows efficient calculation of gradients of the loss with respect to network parameters, which allows the use of many first-order methods. The simplest first-order method is the <em>steepest descent</em> method, which updates the weights at iteration math:<cite>tau</cite> according to the following</p>
<div class="math notranslate nohighlight">
\[\mathbf{w}_i [\tau + 1] = \mathbf{w}_i[\tau] - \eta \left.\frac{\partial L}{\partial \mathbf{w}_i}\right|_{\tau}\]</div>
<p>The polynomial coefficients are updated similarly. Here, <span class="math notranslate nohighlight">\(\eta\)</span> is known as the <em>learning rate</em>, which dictates the step size at each iteration. It is a <em>hyperparameter</em> which needs to be decided manually and tuned depending on application. Other methods of optimization include:</p>
<ol class="arabic simple">
<li><p>Momentum-based steepest descent, where we add a fraction of the previous change in parameters to the current step:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathbf{w}_i [\tau + 1] = \mathbf{w}_i[\tau] - \eta \left.\frac{\partial L}{\partial \mathbf{w}_i}\right|_{\mathbf{w}_i, \alpha_{ij}} - \beta(\mathbf{w}_i [\tau + 1] - \mathbf{w}_i[\tau])\]</div>
<p>This introduces another hyperparameter <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<ol class="arabic simple" start="2">
<li><p>Adaptive learning rate: When the previous change causes the loss to decrease, increase the learning rate by 10%; otherwise, decrease it by 50%. The fractional increase/decrease of the learning rate are also hyperparameters.</p></li>
</ol>
<p>There are also variants of these techniques, such as stochastic gradient descent, which selects random samples to evaluate the gradient at each step. Second order methods (such as Newton-Raphson) can also be used, but backpropagation becomes more complicated, and the inversion of the Hessian can be costly.</p>
<p>In EQ, polynomial neural networks are implemented in the Polynet class.</p>
<p><strong>Code Implementation</strong></p>
<p>We demonstrate the polynet routines through a dataset obtained from [1], concerning the system efficiency of a fan blade parameterized by 25 variables. We are given 548 points in total. We load this data into the arrays X and Y:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">equadratures</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;h_X.dat&#39;</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;h_Y.dat&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, we partition the data into a training set and verification set. We will fit the data on the training set, and evaluate the goodness of fit using the verification set.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_data</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_data</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span><span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">train</span><span class="p">]</span>
<span class="n">ver</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_data</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">train</span><span class="p">])</span>
<span class="n">X_ver</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">ver</span><span class="p">]</span>
<span class="n">Y_ver</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">ver</span><span class="p">]</span>
</pre></div>
</div>
<p>Now, we construct an instance of the Polynet class and call the fit method to optimize the parameters.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Polynet</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">Y_train</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">max_iters</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>  <span class="n">opt</span><span class="o">=</span><span class="s1">&#39;adapt&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">net</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
<p>In this example, we will use a single layer network; “2” refers to the number of hidden units in this layer. If we were to use, say, two layers with 3 perceptrons in each layer, we would put “[3,3]” in this argument. “max_iters” specify the number of maximum iterations to run the optimizer. Since we choose to use adaptive learning rate here, the “learning_rate” parameter specifies the <em>initial</em> learning rate, which will evolve across iterations. “verbose” lets us know the progress of the optimizer by displaying the current loss and learning rate.</p>
<p>After the optimizer is done, we can examine the goodness of fit.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span><span class="n">net</span><span class="o">.</span><span class="n">evaluate_fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Y_ver</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">evaluate_fit</span><span class="p">(</span><span class="n">X_ver</span><span class="p">),</span><span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;prediction&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/result.png"><img alt="../_images/result.png" src="../_images/result.png" style="width: 640.0px; height: 480.0px;" /></a>
</div>
<p>The blue dots show the training data and the orange dots the verification data. Though the fit is not perfect (the cluster of points near the top of the range may have caused some difficulties for the optimizer). The verification data is fit with similar accuracy to the training data, showing that the model has not overfit.</p>
<p><strong>References</strong></p>
<dl class="footnote brackets">
<dt class="label" id="id1"><span class="brackets">1</span></dt>
<dd><p>Seshadri, P., Shahpar, S., Constantine, P., Parks, G., Adams, M. Turbomachinery active subspace performance maps. Journal of Turbomachinery, 140(4), 041003. <a class="reference external" href="http://turbomachinery.asmedigitalcollection.asme.org/article.aspx?articleid=2668256">Paper</a></p>
</dd>
</dl>
</div>


              </div>
              
              
          </main>
          

      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    <footer class="footer mt-5 mt-md-0">
  <div class="container">
    <p>
          &copy; Copyright 2016-2021 by equadratures.org.<br/>
        Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.5.1.<br/>
    </p>
  </div>
</footer>
  </body>
</html>